---
title: "Replicate earlier study"
format: html
---

check what we have done in the past

```{r}
library(tidyverse)

# read and load study data
source("~/RProjects/paper-2023-ICNS/R/osn_read_tardump.R")
source("~/RProjects/paper-2023-ICNS/R/osn_make_nice_names.R")
source("~/RProjects/paper-2023-ICNS/R/coerce_meter_to_feet.R")
source("~/RProjects/paper-2023-ICNS/R/coerce_unixepoch_to_datetime.R")
# subset study data ~ jormungand
source("~/RProjects/paper-2023-ICNS/R/spatial_subset_jormungand.R")
source("~/RProjects/paper-2023-ICNS/R/cast_sf_utils.R")

# loop and trajectory processing
source("~/RProjects/paper-2023-ICNS/R/trj_extract_4d_nest.R")
source("~/RProjects/paper-2023-ICNS/R/trj_icao24_in_polygon.R")
source("~/RProjects/paper-2023-ICNS/R/trj_confirm_arrival_at.R")
source("~/RProjects/paper-2023-ICNS/R/trj_check_orphans.R")

# trajectory cleaning
source("~/RProjects/paper-2023-ICNS/R/trj_fix_altitude.R")
source("~/RProjects/paper-2023-ICNS/R/altitude_outlier_filter.R")
source("~/RProjects/paper-2023-ICNS/R/trj_identify_legs.R")

# runway 
source("~/RProjects/paper-2023-ICNS/R/cast_sf_rwy.R")


source("~/RProjects/paper-2023-ICNS/R/assign_threshold_time2.R")
source("~/RProjects/paper-2023-ICNS/R/identify_threshold_overhead.R")
source("~/RProjects/paper-2023-ICNS/R/add_bearing_to_target.R")
source("~/RProjects/paper-2023-ICNS/R/estimate_threshold_time.R")
source("~/RProjects/paper-2023-ICNS/R/identify_threshold_overhead.R")
source("~/RProjects/paper-2023-ICNS/R/add_landing_time_to_nested_data.R")
source("~/RProjects/paper-2023-ICNS/R/trj_landing_runway.R")

source("~/RProjects/paper-2023-ICNS/R/add_distance_rowwise.R")
source("~/RProjects/paper-2023-ICNS/R/interpolate_lin_4D.R")

# set ggplot theme
ggplot2::theme_set(ggplot2::theme_minimal())
```

Load one day of OSN dump

```{}
#| message: false
pth_to_dumps <- "../__DATA/OSN-dumps"
ptn <- "2019-08-01"

# list downloaded dumps
fns <- list.files(path = pth_to_dumps, pattern = ptn, full.names = TRUE)

# load list of chosen dumps
ds  <- fns |> 
  purrr::map_dfr(.f = read_osn_tar_file) |> 
  make_nice_names_osn()
ds <- ds |> 
  mutate(across(.cols = c("ALT_B","ALT_G"), .fns = coerce_meter_to_feet)) 
ds <- ds |> coerce_unixepoch_to_datetime()
```

Subset for study airports
EGLL, EHAM, LSZH

```{}
aip <- read_csv("./data/aip-arp-rwys.csv", show_col_types = FALSE)

apt <- "LSZH"
arp <- filter(aip, ICAO == apt, REF == "ARP") |> rename(ELEV = GEOM_ALT)
jormungand <- trrrj::polygon_at_distance(c(arp$LON, arp$LAT), d = 205)

svs <- spatial_subset_jormungand(ds, jormungand)

arrow::write_parquet(svs |> sf::st_drop_geometry(), paste0("./data/", apt, "-200NM-20190801.feather"))
```

## Load Heathrow

```{r}
apt <- "EGLL"
fn  <- list.files(path = "./data", pattern = paste0(apt,".*\\.feather"), full.names = TRUE)
```

```{r}
study_loop <- function(.apt, .fn, .debug, ...){
  
  #----------- read airport data -------------------------
  aip <- readr::read_csv("./data/aip-arp-rwys.csv", show_col_types = FALSE) |> 
    dplyr::filter(ICAO == .apt)
  if("GEOM_ALT" %in% names(aip)){
    aip <- aip |> dplyr::rename(ELEV = GEOM_ALT)
    }
  
  arp     <- aip |> dplyr::filter(ICAO == .apt,   REF == "ARP")
  rwys    <- aip |> dplyr::filter(ICAO == .apt, ! REF == "ARP")
  rwy_centerline_ls <- cast_rwy_ctr_line_ls(rwys)
  apt_box <- airport_centerline_box(rwys, ...) 
  
  # ---------------------- load trajectory data --------------------------------
  warning(paste(.apt, " - reading - ", .fn))
  ds <- arrow::read_feather(.fn) %>%
    tibble::as_tibble() |>
    #---- enforce above threshold ~ ELEV arp
    dplyr::filter(ALT_B >= arp$ELEV + 300) |>
     fix_altitude_heuristic() |>
     Alt_Outlier_filter(.alt_var = ALT_B)
  
  #---------------------- check for local traffic ----------------------------
  # subset icao24s inside airport box
  seen_at_airport <- ds |> 
    dplyr::filter(ALT_B <= arp$ELEV + 4000) |> 
    spatial_subset_jormungand(apt_box, ...) |> 
    dplyr::distinct(ICAO24) 
  
  ds <- ds |> 
    dplyr::filter(ICAO24 %in% seen_at_airport$ICAO24) |> 
    identify_trajectory_legs(.max_gap = 5) |> 
    mutate(UID = paste(ICAO24, LEG, sep = "-")) 
  
  phase <- ds |> 
    extract_4d_nest() |> 
    confirm_arrival_at(apt_box, rwy_centerline_ls) |> 
    check_arrival_orphan()
  
  # # add landing time
  # with_ldg_time <-  phase |> 
  #   dplyr::mutate(
  #     study_trj = 
  #     case_when(
  #       ARR & ! ORPHAN ~ pmap(
  #             .l = list(data, UID, RWY)
  #           , .f = ~ add_landing_time_to_nested_data(..1, ..2, ..3, rwys))) 
  #     , TRUE ~ "Departure"
  #   )
}
```

```{r}
gotcha <- study_loop(apt, fn)
```


```{r}
apt <- "EGLL"
aip <- readr::read_csv("./data/aip-arp-rwys.csv", show_col_types = FALSE) |> 
    dplyr::filter(ICAO == apt)
rwys_egll <- aip |> filter(REF != "ARP") |> rename(ELEV = GEOM_ALT)


test_landing_time <- function(phase, rwys = rwys_egll){
  
  arrs <- phase |> filter(ARR & !ORPHAN)
  rest <- phase |> filter(! UID %in% arrs$UID )
  
  with_ldg_time <-  arrs |>
    # estimate and assign landing time to arrival trajectories
    dplyr::mutate(
      study_trj = pmap(
              .l = list(data, UID, RWY)
            , .f = ~ add_landing_time_to_nested_data(..1, ..2, ..3, rwys)
            )
      ) |> 
    # pullout ALDT as separate column
    dplyr::mutate(
      ALDT = map(.x = study_trj
               , .f = ~ filter(.x, MST == "ALDT") |> pull(TIME)
               )
      ) |> tidyr::unnest(ALDT) |> 
    # interpolate to 5sec
    dplyr::mutate(
      study_trj = map(.x = study_trj
                    , .f = ~ interpolate_lin_4D(.x, .source = "OSN") |> 
                             add_time_and_distance_to_go() |> 
                             dplyr::mutate(H3_BIN = h3::geo_to_h3(c(LAT, LON), res = 8))
                    )
      )
  
  return(bind_rows(with_ldg_time, rest))
}

(
  mickey <- gotcha |> test_landing_time()
)
```

```{r}
study_sequences <- mickey |> filter(ARR, !ORPHAN) |> select(UID, RWY, ALDT) |> 
  arrange(RWY, ALDT, UID) |> 
  group_by(RWY) |> 
  mutate(LEAD_DELTA = difftime(ALDT, lag(ALDT, default = first(ALDT)), units = "sec"))
study_sequences
```

```{r}
study_sequences |> filter(LEAD_DELTA < 500) |> 
  ggplot() +
  geom_boxplot(aes(x = RWY, y = LEAD_DELTA))
```

```{r}
study_sequences <- mickey |> filter(ARR, !ORPHAN) |> select(UID, RWY, ALDT) |> 
  arrange(RWY, ALDT, UID) |> 
  group_by(RWY) |> 
  mutate(
     LEAD_DELTA = difftime(ALDT, lag(ALDT, default = first(ALDT)), units = "sec")
    ,INTERACT   = LEAD_DELTA < 240
    ,LEADER     = ifelse(INTERACT, lag(UID), NA)) |> 
  ungroup()
study_sequences
```


write-out tesselation data for reference times

```{r}
# check for date of loaded data
ptn <- "2019-08-01"


tessel_fn <- here::here("data", "EGLL-tessel-db.csv.gz")

extract_tessel_info <- function(.study_trj, ...){
  df <- .study_trj
  df <- df |> dplyr::select(H3_BIN, TIME_2GO, DIST_2GO )
}

make_tessel_db_entry <- function(.study_trj, .ptn = ptn, ...){
  df <- .study_trj |> 
    dplyr::select(UID, RWY, study_trj) |> 
    dplyr::mutate(UID = paste(UID, gsub("-", "", .ptn), sep = "-")) |> 
    dplyr::mutate(
      info = purrr::map(
                .x = study_trj
              , .f = ~ extract_tessel_info(.x)
              )
      ) |> 
    tidyr::unnest(info)
  return(df)
}
```

```{r}
tessel_info <- make_tessel_db_entry(mickey |> filter(ARR, !ORPHAN))


tessel_info
```

```{r}
ti <- tessel_info |> ungroup() |> 
  select(-study_trj) |> 
  group_by(H3_BIN) |> summarise(N = n(), .groups = "drop") |> 
  mutate(
    # add geometry column without naming column (easier than fixing it afterwards)
    h3::h3_to_geo_boundary_sf(H3_BIN)
    ) |> 
  sf::st_as_sf()

p1 <- ggplot() +
  geom_sf(data = ti, aes(fill = N), color = NA) + 
  scale_fill_distiller(palette = "Spectral")

leftright <- 0.3
p2 <- p1 +
  coord_sf( xlim = c(arp_egll$LON - leftright, arp_egll$LON + leftright)
           ,ylim = c(arp_egll$LAT - leftright, arp_egll$LAT + leftright)
           )

library(patchwork)
p1 + p2 +
  plot_layout(guides = 'collect') &
  theme(legend.position = "top")
```

```{r}
if(file.exists(tessel_fn)) { message("Open existing tessel database")
}else{
  message("Create tessel database")
    tessel_info |> 
    dplyr::select(H3_BIN, RWY, TIME_2GO, DIST_2GO, UID ) |> 
    dplyr::ungroup() |> 
    readr::write_csv(tessel_fn)
  }
```

t-digest (for paper to DASC?)
https://git.sr.ht/~hrbrmstr/tdigest

```{r}
tessel_lookup <- readr::read_csv(tessel_fn, show_col_types = FALSE) |> 
  group_by(H3_BIN, RWY) |> 
  summarise( TIME_MIN = min(TIME_2GO), TIME_Q5 = quantile(TIME_2GO, probs = 0.05)
            , DIST_MIN= min(DIST_2GO), DIST_Q5 = quantile(DIST_2GO, probs = 0.05)
            , SAMPLE_N = n())
```





```{r}
study_sequences
```

```{r}
tmp <- mickey |> filter(UID %in% study_sequences$UID[1:4]) |> 
  select(UID, RWY2 = RWY, ALDT, study_trj) |> 
  unnest(study_trj)

ggplot(data = tmp) +
  geom_point(aes(x = TIME, y = ALT_B))
```

spacing deviation (t) = min time (trailer (t)) –    min time (leader (t – s)) 

```{r}
source("~/RProjects/paper-2023-ICNS/pairwise_spacing.R")
```


```{r}

#' Smoothing function 
#' wrapper for slider::slide()

roll_median <- function(.df, .var, .whs, .target_var){
  df <- .df %>% 
    mutate(
     {{.target_var}} := slider::slide_dbl({{.var}}, median, .before = .whs, .after = .whs)
  )
}

```

do for 1 pair
```{r}
rq <- my_pairwise_spacing("400550-0", "89505a-0",.t_trail = 134, .bins = tessel_lookup) 

# can we smooth the results ======================
#' Smoothing function 
#' wrapper for slider::slide()

roll_median <- function(.df, .var, .whs, .target_var){
  df <- .df %>% 
    mutate(
     {{.target_var}} := slider::slide_dbl({{.var}}, median, .before = .whs, .after = .whs)
  )
} #-----------------------------------------------
whs <- 45
rqq <- rq |> group_by(UID, SEQ_UID, RWY) %>% 
  roll_median(MIN_TIME_OFF, whs, SPACING) |> 
  filter(STEP < 2500)

ggplot() + 
  geom_line(data = rq,  aes(x = STEP, y = MIN_TIME_OFF), alpha = 0.5) +
  geom_line(data = rqq, aes(x = STEP, y = SPACING), color = "blue")
```

do for more couples 

```{r}
couples <- study_sequences |> 
  group_by(RWY) |> 
  filter(!is.na(LEADER), LEAD_DELTA <= 300) |> 
  ungroup()
couples

sdp <- couples |> 
  dplyr::select(LEAD_UID = LEADER, TRAIL_UID = UID, T_TRAIL = LEAD_DELTA) |> 
  tibble::rowid_to_column("GID")
sdp

sdp_res <- sdp |> group_by(GID) |> 
  purrr::pmap(.f = ~ my_pairwise_spacing(..2, ..3, ..4, .bins = tessel_lookup)) |> 
  bind_rows() |> 
  tidyr::fill(SEQ_UID, .direction = "up")
#
write_csv(sdp_res, "./data/spd-res-EGLL-20190801.csv.gz")
```

```{r}
sdp_smooth <- sdp_res |> group_by(UID, SEQ_UID, RWY) %>% 
  roll_median(MIN_TIME_OFF, whs, SPACING) |> 
  filter(STEP < 2500) 
sdp_smooth <- sdp_smooth |> mutate(GID = dplyr::cur_group_id())

ggplot() + 
  geom_line(  data = sdp_res |> group_by(UID,SEQ_UID) |> 
                              mutate(GID = dplyr::cur_group_id())
            , aes(x = STEP, y = MIN_TIME_OFF, group = GID), alpha = 0.2) +
  geom_line(data = sdp_smooth, aes(x = STEP, y = SPACING, group = GID), color = "blue", alpha = 0.5)
```

